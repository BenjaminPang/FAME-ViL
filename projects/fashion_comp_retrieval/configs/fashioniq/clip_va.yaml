# FashionIQ data nums: 18000

includes:
- configs/models/composition/clip.yaml

model_config:
  simple_composition:
    compositor:
      type: va
    losses:
    - type: contrastive_loss

dataset_config:
  fashioniq:
    processors:
      text_processor:
        type: copy_sentence
      train_image_processor:
        type: torchvision_transforms
        params:
          transforms:
            - type: RandomResizedCrop
              params:
                size: [224, 224]
                scale: [0.8, 1.0]
                ratio: [0.75, 1.3]
            - RandomHorizontalFlip
            - ToTensor
            - type: Normalize
              params:
                mean: [0.48145466, 0.4578275, 0.40821073]
                std: [0.26862954, 0.26130258, 0.27577711]
      eval_image_processor:
        type: torchvision_transforms
        params:
          transforms:
            - type: Resize
              params:
                size: [256, 256]
            - type: CenterCrop
              params:
                size: [224, 224]
            - ToTensor
            - type: Normalize
              params:
                mean: [0.48145466, 0.4578275, 0.40821073]
                std: [0.26862954, 0.26130258, 0.27577711]

scheduler:
  type: multi_step
  params:
    use_warmup: false
    lr_steps:
    - 4000
    lr_ratio: 0.1
    warmup_iterations: 2810
    warmup_factor: 0.25

optimizer:
  type: adam_w
  params:
    lr: 1e-6
    eps: 1e-8
    weight_decay: 1e-4

evaluation:
  metrics:
    - r@k_fashioniq

training:
  experiment_name: clip_va_fashioniq_maaf
  batch_size: 128
  lr_scheduler: true
  max_updates: 6000
  log_interval: 10
  checkpoint_interval: 5620
  evaluation_interval: 562
  early_stop:
    criteria: fashioniq/r@k_fashioniq/avg
    minimize: false
  wandb:
    enabled: true

run_type: train_val
