dataset_config:
  fashiongen_cls:
    processors:
      text_processor:
        type: clip_tokenizer
        params:
          tokenizer_config:
            type: openai/clip-vit-base-patch16
      train_image_processor:
        type: torchvision_transforms
        params:
          transforms:
            - type: RandomResizedCrop
              params:
                size: [224, 224]
                scale: [0.8, 1.0]
                ratio: [0.75, 1.3]
            - RandomHorizontalFlip
            - ToTensor
            - type: Normalize
              params:
                mean: [0.48145466, 0.4578275, 0.40821073]
                std: [0.26862954, 0.26130258, 0.27577711]
      eval_image_processor:
        type: torchvision_transforms
        params:
          transforms:
            - type: Resize
              params:
                size: [256, 256]
            - type: CenterCrop
              params:
                size: [224, 224]
            - ToTensor
            - type: Normalize
              params:
                mean: [0.48145466, 0.4578275, 0.40821073]
                std: [0.26862954, 0.26130258, 0.27577711]
