dataset_config:
  fashionall:
      data_dir: ${env.data_dir}/datasets
      use_images: true
      use_features: false
      images:
        train:
        - BigFACAD/images
        - Fashion200k/images
        - FashionGen/train
        - PolyvoreOutfits/images
        val:
        - BigFACAD/images
        - Fashion200k/images
        - FashionGen/val
        - PolyvoreOutfits/images
        test:
        - BigFACAD/images
        - Fashion200k/images
        - FashionGen/val
        - PolyvoreOutfits/images
      annotations:
        train:
        - BigFACAD/train_info.json
        - Fashion200k/train_info.json
        - FashionGen/train_info.json
        - PolyvoreOutfits/train_info.json
        val:
        - BigFACAD/val_info.json
        - Fashion200k/test_info.json
        - FashionGen/val_info.json
        - PolyvoreOutfits/val_info.json
        test:
        - BigFACAD/val_info.json
        - Fashion200k/test_info.json
        - FashionGen/val_info.json
        - PolyvoreOutfits/test_info.json
      features:
        train:
        - BigFACAD/image_features
        - Fashion200k/image_features
        - FashionGen/train_features
        - PolyvoreOutfits/train_features
        val:
        - BigFACAD/image_features
        - Fashion200k/image_features
        - FashionGen/val_features
        - PolyvoreOutfits/val_features
        test:
        - BigFACAD/image_features
        - Fashion200k/image_features
        - FashionGen/val_features
        - PolyvoreOutfits/test_features
      processors:
        text_processor:
          type: bert_tokenizer
          params:
            tokenizer_config:
              type: bert-base-uncased
              params:
                do_lower_case: true
            mask_probability: 0
            max_seq_length: 100
        train_image_processor:
          type: torchvision_transforms
          params:
            transforms:
              - type: Resize
                params:
                  size: [256, 256]
              - type: RandomCrop
                params:
                  size: [224, 224]
              - ToTensor
              - type: Normalize
                params:
                  mean: [0.46777044, 0.44531429, 0.40661017]
                  std: [0.12221994, 0.12145835, 0.14380469]
        eval_image_processor:
          type: torchvision_transforms
          params:
            transforms:
              - type: Resize
                params:
                  size: [256, 256]
              - type: CenterCrop
                params:
                  size: [224, 224]
              - ToTensor
              - type: Normalize
                params:
                  mean: [0.46777044, 0.44531429, 0.40661017]
                  std: [0.12221994, 0.12145835, 0.14380469]
