<div align="center">

# FAME-ViL: Multi-Tasking Vision-Language Model for Heterogeneous Fashion Tasks

<a href="https://pytorch.org/get-started/locally/"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white"></a>
<a href="https://mmf.sh/"><img alt="MMF" src="https://img.shields.io/badge/MMF-0054a6?logo=meta&logoColor=white"></a>
[![Conference](http://img.shields.io/badge/CVPR-2023(Highlight)-6790AC.svg)](https://cvpr.thecvf.com/)
[![Paper](http://img.shields.io/badge/Paper-arxiv.2303.02483-B31B1B.svg)](https://arxiv.org/abs/2303.02483)

</div>

## Updates
- :heart_eyes: (21/03/2023) Our FAME-ViL is selected as a **highlight paper** at CVPR 2023! (**Top 2.5%** of 9155 submissions)
- :blush: (12/03/2023) Code released!

Our trained model is available at [Google Drive](https://drive.google.com/file/d/11U5PZU9BE3gQ1dzvJdrav5V2mRNW-pEq/view?usp=sharing).

A detailed README will be updated later. Please stay tuned!
